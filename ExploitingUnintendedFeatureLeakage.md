Exploiting Unintended Feature Leakage in Collaborative Learning

###### abstract

Collaborative learning(federated learning): multiple participants with own training dataset, to build a joint model by training locally and periodically exchanging model updates.

these updates leak unintended information about participants’ training data

develop passive and active inference attacks to exploit this leakage

1. infer the presence (i.e., membership inference). 
2. infer properties



###### Introduction

privacy in collaborative learning .

Key question: **what can be inferred about a participant’s training dataset from the model updates?**

we focus on inferring “unintended” features, i.e., properties that hold for certain subsets of the training data, but not generically for all class members.

**membership inference:**

membership inference: given an exact data point, determine if it was used to train the model.

passive and active membership inference attacks.

collaborative learning presents interesting new avenues for such inferences. 

 **property inference:**

passive and active property inference attacks that allow an adversarial participant in collaborative learning to infer properties of other participants’ training data that are not true of the class as a whole.

 For example,when the model is trained on the LFW dataset to recognize gender or race, we infer whether people in the training photos wear glasses—a property that is uncorrelated with the main task. By contrast,prior property inference attacks [2, 25] infer only properties that characterize an entire class.



Our key observation: deep-learning models learn unintended features which are independent of the task being learned.

We also demonstrate that an active adversary can use multi-task learning to trick the joint model into learning a better internal separation of the features that are of interest to him and thus extract even more information. 



**Some of our inference attacks have direct privacy implications**:

we infer that a certain person appears in a single training batch even if half of the photos in the batch depict other people.

 we infer the specialty of the doctor being reviewed with perfect accuracy.

 we identify the author even if their reviews account for less than a third of the batch.



We also measure the performance of our attacks vis-`a-vis the number of participants 



Finally, we evaluate possible defenses



###### Background

**ML:**

we focus on the supervised learning of classiﬁcation tasks.

SGD, hyperparameters, DL

**CL:**

partition the training dataset, concurrently train separate models on each subset, and exchange parameters via a parameter server.



**Collaborative learning with synchronized gradient updates.** 

Model: aggregate local gradient $g$

with privacy:



**Federated learning with model averaging**

we set C to 1

Model:  aggregate local model $\theta$



The convergence rate of both collaborative learning approaches heavily depends on the learning task and the hyperparameters (e.g., number of participants and batch size).



###### Reasoning about Privacy in Machine Learning 



**Inferring class representatives**

Given black-box access to a classiﬁer model, model inversion attacks [16] infer features that characterize each class, making it possible to construct representatives of these classes.

In the special case, ...

Note that neither technique reconstructs actual training inputs.



**Inferring membership in training data**

Membership inference attacks against aggregate statistics

black-box membership inference against ML models

The ability of an adversary to infer the presence of a speciﬁc data point in a training dataset constitutes an immediate privacy threat if the dataset is in itself sensitive.



 **Inferring properties of training data** 

In collaborative and federated learning, participants’ training data may not be identically distributed.

Prior work aimed to infer properties that characterize an entire class.

By contrast,we aim to infer properties that are true of a subset of the training inputs but not of the class as a whole.  We especially focus on the properties that are independent of the class’s characteristic features.

 We infer single-batch properties.

We also infer when a property appears in the training data.



###### Inference Attacks

**Threat Model**

K participants : adversary, target participant, honest participants 

The updates that adversary observes and uses for inference depend on both K and how collaborative training is done. 

inputs to inference algorithms: ...



**Overview of the attacks**

1. $\Delta\theta_t=\theta_t-\theta_{t-1}$
2. $\Delta \theta_t -\Delta \theta_t^{adv}$



**Leakage from the embedding layer :**

the non-zero gradients of the embedding layer reveal which words appear in a batch.

**Leakage from the gradients :**

Gradients of a given layer are computed using this layer’s features and the error from the layer above. 

Observations of gradient updates can thus be used to infer feature values, which are in turn based on the participants’ private training data



**Membership inference** 

The adversary can use this to decide whether $r$ was a member or not



**Passive property inference**

This produces labeled examples, which enable the adversary to train a binary batch property classiﬁer that determines if the observed updates are based on the data with or without the property. 

This attack is passive, i.e., the adversary observes the updates and performs inference without changing anything in the local or global collaborative training procedure. 

*Batch property classiﬁer* : ...



**Active property inference**

multi-task learning

The adversary extends his local copy of the collaboratively trained model with an augmented property classiﬁer connected to the last layer. He trains this model to simultaneously perform well on the main task and recognize batch properties. 

The only difference with the passive attack is that this adversary performs additional local computations and submits the resulting values into the collaborative learning protocol.



###### Datasets and model architectures



###### Two-Party Experiments 



###### Multi-Party Experiments 



###### Defenses

**Todo**



###### Limitations of the attacks

Auxiliary data, 

Number of participants, 

Undetectable properties, 

Attribution of inferred properties



###### Related Work

Privacy-preserving distributed learning:

Membership inference

Other attacks on machine learning models

